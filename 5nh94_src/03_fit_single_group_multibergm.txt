#!/bin/bash
#!#######################################
#!#### SLURM CONFIGURATION OPTIONS ######
#!#######################################

#SBATCH -J youngK3
#SBATCH --ntasks=20
#SBATCH --time=02-00:00:00

#!################################
#!#### LOAD REQUIRED MODULES #####
#!################################

module load r

#!#########################
#!#### COMMAND TO RUN #####
#!#########################

### Vary 'constK3' between 'constK3','constK5','propK3','propK5'
### The next two parameters control the MCMC proposals, varied to yield acceptance
### rates of between 0.2 and 0.5.
### The fourth parameter specifies the number of MCMC iterations
CMD="Rscript 03_fit_single_group_multibergm.R constK3 0.02 0.005 31000 $SLURM_NTASKS"

#! Number of nodes and tasks per node allocated by SLURM (do not change):
numnodes=$SLURM_JOB_NUM_NODES
numtasks=$SLURM_NTASKS
mpi_tasks_per_node=$(echo "$SLURM_TASKS_PER_NODE" | sed -e  's/^\([0-9][0-9]*\).*$/\1/')

#! Work directory (i.e. where the job will run):
workdir="$SLURM_SUBMIT_DIR"  # The value of SLURM_SUBMIT_DIR sets workdir to the directory
                             # in which sbatch is run.

#! Number of MPI tasks to be started by the application per node and in total (do not change):
np=$[${numnodes}*${mpi_tasks_per_node}]

cd $workdir
JOBID=$SLURM_JOB_ID
eval $CMD
